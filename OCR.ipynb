{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-TlFiXQwM6N",
        "outputId": "f32e8cc7-8de3-44e8-a027-16f9594b70a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Requirement already satisfied: install in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytelegrambotapi in /usr/local/lib/python3.10/dist-packages (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytelegrambotapi) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "mkdir: cannot create directory ‘img’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --no-cache-dir transformers sentencepiece\n",
        "!pip install pip install pytelegrambotapi\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!mkdir img\n",
        "#!unzip archivet.zip -d dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import csv\n",
        "import evaluate\n",
        "import time"
      ],
      "metadata": {
        "id": "r0kGgMMEdtp8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLV0BJfywO6o",
        "outputId": "b7d8772f-a34c-48e5-929d-9317b2fda0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-large-handwritten', use_fast=False)\n",
        "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-large-handwritten')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor1 = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten', use_fast=False)\n",
        "model1 = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRQRJXojMLSN",
        "outputId": "c7affbe5-e155-4cbd-f548-811c7b23ba20"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processor2 = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten', use_fast=False)\n",
        "model2 = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zk_bLu0hs6K",
        "outputId": "9ce4b985-2b27-4148-9810-34c8b62c37c4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ypbaoUl5R-TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2991feaa-47af-4f99-e8e9-596727b0423e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def model_answer(path):\n",
        "    mes = \"\"\n",
        "    try:\n",
        "      image = cv2.imread(path)\n",
        "      gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "      blur = cv2.medianBlur(gray, 5)\n",
        "      thresh = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV,11,8)\n",
        "      kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n",
        "      dilate = cv2.dilate(thresh, kernel, iterations=6)\n",
        "      cnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
        "      #cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n",
        "      cnts = cnts[-1:0:-1]\n",
        "      for c in cnts:\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        ROI = image[y:y+h, x:x+w]\n",
        "        im_pil = Image.fromarray(ROI)\n",
        "        pixel_values = processor(images=im_pil, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "        generated_ids = model.generate(pixel_values)\n",
        "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        generated_text = generated_text.replace('.', '')\n",
        "        generated_text = generated_text.replace('#', '')\n",
        "        generated_text = generated_text.replace('_', '')\n",
        "        generated_text = generated_text.replace('0', '')\n",
        "        generated_text = generated_text.replace('\"', '')\n",
        "        if len(generated_text) < 2 and generated_text != 'a':\n",
        "          generated_text = \"\"\n",
        "        if generated_text != \"\":\n",
        "          mes += generated_text + \"\\n\"\n",
        "      if mes == \"\":\n",
        "        mes = \"Не нашел текста :(\"\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      mes = \"Что-то пошло не так :(\"\n",
        "    return mes\n",
        "\n",
        "\n",
        "bot = telebot.TeleBot(\"\")\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "    bot.send_message(message.chat.id, \"Отправьте картинку\")\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def other_text(message):\n",
        "    bot.send_message(message.chat.id, \"Отправьте картинку\")\n",
        "\n",
        "@bot.message_handler(content_types=['photo'])\n",
        "def get_img(message):\n",
        "    msg = bot.send_message(message.chat.id, \"Идет обработка...\")\n",
        "    file_id = message.photo[-1].file_id\n",
        "    file_info = bot.get_file(file_id)\n",
        "    download_file = bot.download_file(file_info.file_path)\n",
        "    path = f'./img/img{message.chat.id}.{file_info.file_path.split(\".\")[-1]}'\n",
        "    with open(path, 'wb') as file:\n",
        "        file.write(download_file)\n",
        "    bot.edit_message_text(chat_id=message.chat.id, message_id=msg.message_id, text=model_answer(path))\n",
        "    bot.send_message(message.chat.id, \"Отправьте картинку\")\n",
        "\n",
        "bot.polling(non_stop=True, interval=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"written_name_test_v2.csv\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    check_dict = dict((rows[0],rows[1]) for rows in reader)\n",
        "predictions = []\n",
        "references = []\n",
        "for file in check_dict.keys():\n",
        "  image = Image.open(f\"./dataset/test/{file}\").convert(\"RGB\")\n",
        "  pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model.generate(pixel_values)\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  predictions.append(generated_text)\n",
        "  references.append([check_dict[file]])"
      ],
      "metadata": {
        "id": "dEXs6brM1-E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"written_name_test_v2.csv\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    check_dict = dict((rows[0],rows[1]) for rows in reader)\n",
        "predictions1 = []\n",
        "references1 = []\n",
        "for file in check_dict.keys():\n",
        "  image = Image.open(f\"./dataset/test/{file}\").convert(\"RGB\")\n",
        "  pixel_values = processor1(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model1.generate(pixel_values)\n",
        "  generated_text = processor1.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  predictions1.append(generated_text)\n",
        "  references1.append([check_dict[file]])"
      ],
      "metadata": {
        "id": "I5Ak0VxCN4M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"written_name_test_v2.csv\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    check_dict = dict((rows[0],rows[1]) for rows in reader)\n",
        "predictions2 = []\n",
        "references2 = []\n",
        "for file in check_dict.keys():\n",
        "  image = Image.open(f\"./dataset/test/{file}\").convert(\"RGB\")\n",
        "  pixel_values = processor2(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model2.generate(pixel_values)\n",
        "  generated_text = processor2.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  predictions2.append(generated_text)\n",
        "  references2.append([check_dict[file]])"
      ],
      "metadata": {
        "id": "U0WGw_dbi57W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(predictions)):\n",
        "  predictions[i] = predictions[i].replace('.', '')\n",
        "  predictions[i] = predictions[i].replace(' ', '')\n",
        "  predictions[i] = predictions[i].replace('#', '')\n",
        "  predictions[i] = predictions[i].replace('_', '')\n",
        "  predictions[i] = predictions[i].upper()"
      ],
      "metadata": {
        "id": "bsS_HEhl5Yv0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(predictions1)):\n",
        "  predictions1[i] = predictions1[i].replace('.', '')\n",
        "  predictions1[i] = predictions1[i].replace(' ', '')\n",
        "  predictions1[i] = predictions1[i].replace('#', '')\n",
        "  predictions1[i] = predictions1[i].replace('_', '')\n",
        "  predictions1[i] = predictions1[i].upper()"
      ],
      "metadata": {
        "id": "UubBXftlQNnS"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(predictions2)):\n",
        "  predictions2[i] = predictions2[i].replace('.', '')\n",
        "  predictions2[i] = predictions2[i].replace(' ', '')\n",
        "  predictions2[i] = predictions2[i].replace('#', '')\n",
        "  predictions2[i] = predictions2[i].replace('_', '')\n",
        "  predictions2[i] = predictions2[i].upper()"
      ],
      "metadata": {
        "id": "FUeLPfiljGUn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions, references=references)\n",
        "print(\"large\", results['precisions'][0]*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYattKA08zbu",
        "outputId": "637cb622-9981-428c-cad2-16c659a00ade"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "large 10.606060606060606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions1, references=references1)\n",
        "print(\"base\", results['precisions'][0]*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZYkvkiSOMtx",
        "outputId": "e74343ed-99e8-4b26-a748-4da000b8b22c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base 3.9215686274509802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "results = bleu.compute(predictions=predictions2, references=references2)\n",
        "print(\"small\", results['precisions'][0]*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3crBM2NejOj7",
        "outputId": "dbcfe98e-255c-4f9c-d310-99ab7e962754"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "small 11.11111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"written_name_test_v2.csv\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    check_dict = dict((rows[0],rows[1]) for rows in reader)\n",
        "times = []\n",
        "times1 = []\n",
        "times2 = []\n",
        "for file in check_dict.keys():\n",
        "  image = Image.open(f\"./dataset/test/{file}\").convert(\"RGB\")\n",
        "  time1 = time.time()\n",
        "  pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model.generate(pixel_values)\n",
        "  generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  time2 = time.time()\n",
        "  times.append(time2 - time1)\n",
        "  time1 = time.time()\n",
        "  pixel_values = processor1(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model1.generate(pixel_values)\n",
        "  generated_text = processor1.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  time2 = time.time()\n",
        "  times1.append(time2 - time1)\n",
        "  time1 = time.time()\n",
        "  pixel_values = processor2(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "  generated_ids = model2.generate(pixel_values)\n",
        "  generated_text = processor2.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  time2 = time.time()\n",
        "  times2.append(time2 - time1)\n",
        "print(\"large\", sum(times)/float(len(times)))\n",
        "print(\"base\", sum(times1)/float(len(times1)))\n",
        "print(\"small\", sum(times2)/float(len(times2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3eLXmSxQ-IQ",
        "outputId": "23d17ac8-102a-4acd-fe7b-25aae043fd24"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "large 14.50441312789917\n",
            "base 10.60425066947937\n",
            "small 0.8907990455627441\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}